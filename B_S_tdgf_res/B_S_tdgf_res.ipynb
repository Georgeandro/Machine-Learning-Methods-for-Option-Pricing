{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZlRgeRRFbL-",
        "outputId": "8c602205-e539-41df-83a7-79c1337c1f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "# Check for CUDA availability and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Heston model parameters\n",
        "r = 0.1      # risk-free rate\n",
        "sigma = 0.25    # initial variance\n",
        "\n",
        "# Domain boundaries\n",
        "x_min, x_max = 0.1, 3.0        # moneyness range [0.1, 3.0]\n",
        "T = 1.0                       # maturity (1 year)\n",
        "N_steps = 100                 # number of time steps\n",
        "dt = T / N_steps              # time step size\n",
        "\n",
        "# Training settings\n",
        "train_stages = 2000           # training iterations per time step\n",
        "M_samples = 1800              # samples per training iteration (600*(d+1) with d=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class ResidualLayer(torch.nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.linear = torch.nn.Linear(dim, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = torch.tanh(self.linear(x))\n",
        "        return h + x\n",
        "\n",
        "class ResidualNet(torch.nn.Module):\n",
        "    def __init__(self, layer_width, input_dim, r=0.0, t=0.0, CP=1, xp=2.0):\n",
        "        super(ResidualNet, self).__init__()\n",
        "        self.xp = xp\n",
        "        self.r = r\n",
        "        self.t = t\n",
        "        self.CP = CP\n",
        "        self.W = torch.nn.Linear(input_dim, layer_width)\n",
        "        torch.nn.init.xavier_uniform_(self.W.weight)\n",
        "        self.res1 = ResidualLayer(layer_width)\n",
        "        self.res2 = ResidualLayer(layer_width)\n",
        "        self.res3 = ResidualLayer(layer_width)\n",
        "        self.W4 = torch.nn.Linear(layer_width, 1, bias=False)\n",
        "        self.W4.weight = torch.nn.Parameter(torch.ones((1, layer_width)))\n",
        "\n",
        "    def forward_main(self, X):\n",
        "        payoff = torch.max(torch.tensor(0.0).to('cuda'), self.CP * (self.S - math.exp(-self.r * self.t)))\n",
        "        if self.t == 0.0:\n",
        "            return payoff\n",
        "        else:\n",
        "            X1 = torch.tanh(self.W(X))\n",
        "            X2 = self.res1(X1)\n",
        "            X3 = self.res2(X2)\n",
        "            X4 = self.res3(X3)\n",
        "            return payoff + torch.log(torch.exp(self.W4(X4)) + 1)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        x = torch.concat([*args], 1)\n",
        "        self.S = x[:, 0, None]\n",
        "        if self.CP == 1:\n",
        "            S_box = torch.where(self.S > self.xp, self.xp * torch.ones_like(self.S), self.S).requires_grad_()\n",
        "            S_diff = (self.S - S_box).requires_grad_()\n",
        "            x[:, 0, None] = S_box\n",
        "            return self.forward_main(x) + S_diff\n",
        "        else:\n",
        "            return self.forward_main(x)\n"
      ],
      "metadata": {
        "id": "wuX2FgrsFfTZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option parameters\n",
        "T = 1.0\n",
        "r = 0.05\n",
        "sigma = 0.25\n",
        "option = 'call'\n",
        "CP = 1 if option == 'call' else -1\n",
        "name = 'BS_' + option\n",
        "xp = 2.0\n",
        "\n",
        "# Neural network parameters\n",
        "nodes_per_layer = 50\n",
        "d = 1  # Input dimension: S\n",
        "\n",
        "# Sampling parameters\n",
        "nSim_dim = 600\n",
        "nSim_TDGF = nSim_dim * d\n",
        "S_low = 0.01\n",
        "S_high = 3.0\n",
        "Omega = S_high - S_low\n",
        "\n",
        "# TDGF parameters\n",
        "N_t = 100\n",
        "h = T / N_t\n",
        "\n",
        "# Training parameters\n",
        "sampling_stages_DNM = 2000\n",
        "\n",
        "# Terminal payoff function\n",
        "def Phi(x):\n",
        "    return torch.max(torch.tensor(0.0).to('cuda'), CP * (x - 1.0))\n",
        "\n",
        "# Sampling function\n",
        "def sampler(nSim):\n",
        "    t = T * torch.rand([nSim, 1])\n",
        "    S = S_low + torch.rand([nSim, 1]) * Omega\n",
        "    return t.requires_grad_().to('cuda'), S.requires_grad_().to('cuda')\n",
        "\n",
        "# Loss function for pretraining (terminal condition)\n",
        "def lossTerminal(S_int):\n",
        "    f = DNM_model(S_int)\n",
        "    return torch.mean((f - Phi(S_int))**2)\n",
        "\n",
        "# Loss function for TDGF\n",
        "def lossDNM(S_int):\n",
        "    f = DNM_model(S_int)\n",
        "    f_S = torch.autograd.grad(f, S_int, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
        "    a = 0.5 * sigma**2 * S_int**2\n",
        "    Lagrangian = 0.5 * (a * f_S**2 + r * f**2)\n",
        "    G1 = torch.mean(Lagrangian)\n",
        "    f_old = old_model(S_int)\n",
        "    old_S = torch.autograd.grad(f_old, S_int, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
        "    b = (sigma**2 - r) * S_int\n",
        "    F = b * old_S\n",
        "    G2 = torch.mean(F * f)\n",
        "    G3 = torch.mean((f - f_old)**2)\n",
        "    return 0.5 * G3 + h * (G1 + G2)"
      ],
      "metadata": {
        "id": "lT7hVFRIFj-Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as st\n",
        "\n",
        "# Black-Scholes analytical solution\n",
        "def black_scholes_price(S, K, t, T, r, sigma, option_type):\n",
        "    \"\"\"\n",
        "    Calculates the Black-Scholes option price.\n",
        "\n",
        "    Args:\n",
        "        S (float or torch.Tensor): Underlying price.\n",
        "        K (float): Strike price.\n",
        "        t (float): Current time.\n",
        "        T (float): Maturity time.\n",
        "        r (float): Risk-free rate.\n",
        "        sigma (float): Volatility.\n",
        "        option_type (str): 'call' or 'put'.\n",
        "\n",
        "    Returns:\n",
        "        float or torch.Tensor: Black-Scholes option price.\n",
        "    \"\"\"\n",
        "    # Ensure S is a tensor for calculations\n",
        "    if not isinstance(S, torch.Tensor):\n",
        "        S = torch.tensor(S, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Ensure T-t is a tensor for torch.sqrt\n",
        "    time_to_maturity = T - t\n",
        "    if not isinstance(time_to_maturity, torch.Tensor):\n",
        "        time_to_maturity = torch.tensor(time_to_maturity, dtype=torch.float32, device=S.device) # Use same device as S\n",
        "\n",
        "    # Handle potential division by zero if time_to_maturity is very small or zero\n",
        "    # Add a small epsilon to avoid issues if time_to_maturity is exactly zero\n",
        "    epsilon = 1e-9\n",
        "    sqrt_time_to_maturity = torch.sqrt(time_to_maturity + epsilon)\n",
        "\n",
        "    d1 = (torch.log(S / K) + (r + 0.5 * sigma**2) * time_to_maturity) / (sigma * sqrt_time_to_maturity)\n",
        "    d2 = d1 - sigma * sqrt_time_to_maturity\n",
        "\n",
        "    # Use torch.distributions.normal.cdf for tensor-compatible CDF\n",
        "    # Need to import torch.distributions\n",
        "    from torch.distributions import Normal\n",
        "    normal_dist = Normal(torch.tensor([0.0], device=S.device), torch.tensor([1.0], device=S.device))\n",
        "\n",
        "    if option_type == 'call':\n",
        "        price = S * normal_dist.cdf(d1) - K * torch.exp(-r * time_to_maturity) * normal_dist.cdf(d2)\n",
        "    elif option_type == 'put':\n",
        "        price = K * torch.exp(-r * time_to_maturity) * normal_dist.cdf(-d2) - S * normal_dist.cdf(-d1)\n",
        "    else:\n",
        "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
        "\n",
        "    return price"
      ],
      "metadata": {
        "id": "8LOScSWD7B36"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "# Ensure necessary classes and functions are available (assuming they are defined in previous cells)\n",
        "# from . import DGMNet, sampler, lossTerminal, lossDNM, device, name, h, T, N_t, nSim_TDGF, sampling_stages_DNM, S_low, S_high\n",
        "\n",
        "# Train TDGF network\n",
        "start_time = time.time()\n",
        "\n",
        "# Assuming DNM_model is already defined and initialized\n",
        "# For demonstration, let's initialize it if it's not already\n",
        "try:\n",
        "    DNM_model\n",
        "except NameError:\n",
        "    # These parameters should match your model initialization\n",
        "    nodes_per_layer = 50\n",
        "    d = 1\n",
        "    r = 0.05\n",
        "    T = 1.0 # Make sure T is defined\n",
        "    CP = 1 # Make sure CP is defined\n",
        "    xp = 2.0 # Make sure xp is defined\n",
        "    DNM_model = ResidualNet(nodes_per_layer, d, r=r, t=0.0, CP=CP, xp=xp).to(device)\n",
        "\n",
        "\n",
        "# Pretraining to approximate terminal condition\n",
        "print(\"Starting pretraining...\")\n",
        "optimizer = torch.optim.Adam(DNM_model.parameters(), 3e-4)\n",
        "# For pretraining, we need samples at time T. The current sampler provides t from [0,T].\n",
        "# Let's assume we only use the S values and conceptually consider them as S at time T.\n",
        "# If more accurate sampling at T is needed, modify the sampler for the pretraining phase.\n",
        "_, S_pretrain = sampler(sampling_stages_DNM * nSim_TDGF) # Sample a large dataset for pretraining\n",
        "S_pretrain = S_pretrain.to(device).requires_grad_() # Ensure on device and requires grad\n",
        "\n",
        "pretrain_batch_size = nSim_TDGF\n",
        "num_pretrain_batches = (sampling_stages_DNM * nSim_TDGF) // pretrain_batch_size\n",
        "\n",
        "for j in range(num_pretrain_batches):\n",
        "    start_idx = j * pretrain_batch_size\n",
        "    end_idx = (j + 1) * pretrain_batch_size\n",
        "    S_batch = S_pretrain[start_idx:end_idx]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = lossTerminal(S_batch) # Make sure lossTerminal is defined\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (j + 1) % 100 == 0: # Print loss every 100 batches\n",
        "        print(f\"Pretraining batch {j+1}/{num_pretrain_batches}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "# Handle remaining pretraining samples\n",
        "if (sampling_stages_DNM * nSim_TDGF) % pretrain_batch_size != 0:\n",
        "    start_idx = num_pretrain_batches * pretrain_batch_size\n",
        "    S_batch = S_pretrain[start_idx:]\n",
        "    optimizer.zero_grad()\n",
        "    loss = lossTerminal(S_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Pretraining final batch, Loss: {loss.item():.6f}\")\n",
        "\n",
        "print(\"Pretraining finished.\")\n",
        "\n",
        "# Create the 'weights' directory if it doesn't exist\n",
        "os.makedirs('weights', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True) # Create 'plots' directory\n",
        "\n",
        "# Save the pretrained model (which is at t=0.0)\n",
        "filename_initial = 'weights/' + name + '_t=0.0' # Make sure name is defined\n",
        "# Set the model time to 0.0 before saving the pretrained model\n",
        "DNM_model.t = 0.0\n",
        "# torch.save(DNM_model.state_dict(), filename_initial) # Save state_dict - Keep this line\n",
        "torch.save(DNM_model.state_dict(), filename_initial) # Save state_dict\n",
        "\n",
        "# Load the pretrained model as the initial old_model for the first time step training\n",
        "# Instead of loading the OrderedDict, instantiate a new model and load the state_dict\n",
        "old_model = ResidualNet(nodes_per_layer, d, r=r, t=0.0, CP=CP, xp=xp).to(device) # Instantiate a new model and move it to the device\n",
        "# Load the state dictionary into the newly created model instance\n",
        "old_model.load_state_dict(torch.load(filename_initial, weights_only=False))\n",
        "\n",
        "# Ensure old_model's time parameter is set to the time corresponding to the saved model\n",
        "old_model.t = 0.0\n",
        "\n",
        "# Ensure K (Strike price) is defined for the Black-Scholes calculation\n",
        "# Based on the Phi function `CP * (x - 1.0)`, the strike price K appears to be 1.0.\n",
        "K = 1.0\n",
        "\n",
        "# Train over time steps\n",
        "print(\"\\nStarting time step training...\")\n",
        "# Make sure h and N_t are defined\n",
        "for i, curr_t_tensor in enumerate(torch.linspace(h, T, N_t)):\n",
        "    curr_t = curr_t_tensor.item() # Get the scalar value of the current time\n",
        "    print(f\"\\nTraining for time step {i+1}/{N_t}, t = {curr_t:.4f}\")\n",
        "\n",
        "    # Set the current time in the model being trained\n",
        "    DNM_model.t = curr_t\n",
        "\n",
        "    # --- Sample the whole dataset for this time step ---\n",
        "    # Total samples for this time step training\n",
        "    total_samples_for_timestep = sampling_stages_DNM * nSim_TDGF\n",
        "    # Sample a large batch of S values.\n",
        "    _, S_sampled_for_timestep = sampler(total_samples_for_timestep)\n",
        "    # Ensure the sampled data is on the correct device and requires gradients\n",
        "    S_sampled_for_timestep = S_sampled_for_timestep.to(device).requires_grad_()\n",
        "\n",
        "    optimizer = torch.optim.Adam(DNM_model.parameters(), 3e-4)\n",
        "\n",
        "    # --- Iterate through the pre-sampled data in batches ---\n",
        "    batch_size = nSim_TDGF\n",
        "    num_batches = total_samples_for_timestep // batch_size\n",
        "\n",
        "    for j in range(num_batches):\n",
        "        # Get a batch of data\n",
        "        start_idx = j * batch_size\n",
        "        end_idx = (j + 1) * batch_size\n",
        "        S_batch = S_sampled_for_timestep[start_idx:end_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Pass the current batch of S to lossDNM. The lossDNM function uses the current time 'DNM_model.t'\n",
        "        # and the 'old_model.t' internally.\n",
        "        loss = lossDNM(S_batch) # Make sure lossDNM is defined\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (j + 1) % (500 // (total_samples_for_timestep // (sampling_stages_DNM * batch_size))) == 0: # Adjust printing frequency based on batches\n",
        "             print(f\"  Time step {i+1}, Batch {j+1}/{num_batches}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    # Handle any remaining samples if total_samples_for_timestep is not perfectly divisible by batch_size\n",
        "    if total_samples_for_timestep % batch_size != 0:\n",
        "        start_idx = num_batches * batch_size\n",
        "        S_batch = S_sampled_for_timestep[start_idx:]\n",
        "        optimizer.zero_grad()\n",
        "        loss = lossDNM(S_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"  Time step {i+1}, Final batch, Loss: {loss.item():.6f}\")\n",
        "\n",
        "\n",
        "    # Save current model state_dict\n",
        "    filename_curr = 'weights/' + name + '_t=' + str(round(curr_t, 4)) # Use 4 decimal places for time\n",
        "    torch.save(DNM_model.state_dict(), filename_curr)\n",
        "\n",
        "    # Load the just-saved model as old_model for the *next* time step\n",
        "    # Instantiate a new model and load the state_dict\n",
        "    old_model = ResidualNet(nodes_per_layer, d, r=r, t=curr_t, CP=CP, xp=xp).to(device) # Instantiate new model with updated time\n",
        "    old_model.load_state_dict(torch.load(filename_curr, weights_only=False)) # Load the state dict into the new model\n",
        "\n",
        "    # Plotting the option price vs S for the current time step\n",
        "    print(\"Generating plot...\")\n",
        "    S_plot = torch.linspace(S_low, S_high, 100).unsqueeze(1).to(device).requires_grad_()\n",
        "    with torch.no_grad(): # Disable gradient calculation for plotting\n",
        "        # Set the time in the model for plotting (already set for training, but good practice)\n",
        "        DNM_model.t = curr_t\n",
        "        # The forward pass of DGMNet expects inputs as *args. S_plot is the first argument.\n",
        "        # If your model is only taking S as input (d=1), this should work.\n",
        "        # If your model needs time as an input (d=2 for t, S), the forward pass and the\n",
        "        # DGMNet initialization (input_dim) would need adjustment.\n",
        "        option_price = DNM_model(S_plot)\n",
        "\n",
        "        tau = T-curr_t\n",
        "        # Calculate Black-Scholes price at the current time curr_t\n",
        "        bs_price = black_scholes_price(S_plot, K, tau, T, r, sigma, option)\n",
        "\n",
        "\n",
        "    # Detach the tensors from the computation graph before converting to NumPy\n",
        "    S_plot_np = S_plot.cpu().squeeze().detach().numpy()\n",
        "    option_price_np = option_price.cpu().squeeze().detach().numpy()\n",
        "    bs_price_np = bs_price.cpu().squeeze().detach().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(S_plot_np, option_price_np, label=f'DGMNet t = {curr_t:.4f}')\n",
        "    plt.plot(S_plot_np, bs_price_np, label=f'Black-Scholes t = {curr_t:.4f}', linestyle='--')\n",
        "    plt.xlabel('Underlying Price (S)')\n",
        "    plt.ylabel('Option Price')\n",
        "    plt.title(f'{name} Price vs S at t = {curr_t:.4f}')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plot_filename = f'plots/{name}_t={round(curr_t, 4)}.png'\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {plot_filename}\")\n",
        "\n",
        "\n",
        "print(\"\\nTime step training finished.\")\n",
        "end_time = time.time()\n",
        "print(f\"Total training time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Za7CjFc-3j_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b666872-6076-4451-eb61-124cad9bf069"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pretraining...\n",
            "Pretraining batch 100/2000, Loss: 0.000000\n",
            "Pretraining batch 200/2000, Loss: 0.000000\n",
            "Pretraining batch 300/2000, Loss: 0.000000\n",
            "Pretraining batch 400/2000, Loss: 0.000000\n",
            "Pretraining batch 500/2000, Loss: 0.000000\n",
            "Pretraining batch 600/2000, Loss: 0.000000\n",
            "Pretraining batch 700/2000, Loss: 0.000000\n",
            "Pretraining batch 800/2000, Loss: 0.000000\n",
            "Pretraining batch 900/2000, Loss: 0.000000\n",
            "Pretraining batch 1000/2000, Loss: 0.000000\n",
            "Pretraining batch 1100/2000, Loss: 0.000000\n",
            "Pretraining batch 1200/2000, Loss: 0.000000\n",
            "Pretraining batch 1300/2000, Loss: 0.000000\n",
            "Pretraining batch 1400/2000, Loss: 0.000000\n",
            "Pretraining batch 1500/2000, Loss: 0.000000\n",
            "Pretraining batch 1600/2000, Loss: 0.000000\n",
            "Pretraining batch 1700/2000, Loss: 0.000000\n",
            "Pretraining batch 1800/2000, Loss: 0.000000\n",
            "Pretraining batch 1900/2000, Loss: 0.000000\n",
            "Pretraining batch 2000/2000, Loss: 0.000000\n",
            "Pretraining finished.\n",
            "\n",
            "Starting time step training...\n",
            "\n",
            "Training for time step 1/100, t = 0.0100\n",
            "  Time step 1, Batch 500/2000, Loss: 0.000849\n",
            "  Time step 1, Batch 1000/2000, Loss: 0.000854\n",
            "  Time step 1, Batch 1500/2000, Loss: 0.000912\n",
            "  Time step 1, Batch 2000/2000, Loss: 0.000805\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.01.png\n",
            "\n",
            "Training for time step 2/100, t = 0.0200\n",
            "  Time step 2, Batch 500/2000, Loss: 0.000931\n",
            "  Time step 2, Batch 1000/2000, Loss: 0.000892\n",
            "  Time step 2, Batch 1500/2000, Loss: 0.000886\n",
            "  Time step 2, Batch 2000/2000, Loss: 0.000863\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.02.png\n",
            "\n",
            "Training for time step 3/100, t = 0.0300\n",
            "  Time step 3, Batch 500/2000, Loss: 0.000842\n",
            "  Time step 3, Batch 1000/2000, Loss: 0.000827\n",
            "  Time step 3, Batch 1500/2000, Loss: 0.000976\n",
            "  Time step 3, Batch 2000/2000, Loss: 0.000926\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.03.png\n",
            "\n",
            "Training for time step 4/100, t = 0.0400\n",
            "  Time step 4, Batch 500/2000, Loss: 0.000936\n",
            "  Time step 4, Batch 1000/2000, Loss: 0.000885\n",
            "  Time step 4, Batch 1500/2000, Loss: 0.000921\n",
            "  Time step 4, Batch 2000/2000, Loss: 0.000842\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.04.png\n",
            "\n",
            "Training for time step 5/100, t = 0.0500\n",
            "  Time step 5, Batch 500/2000, Loss: 0.000875\n",
            "  Time step 5, Batch 1000/2000, Loss: 0.000874\n",
            "  Time step 5, Batch 1500/2000, Loss: 0.000892\n",
            "  Time step 5, Batch 2000/2000, Loss: 0.000851\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.05.png\n",
            "\n",
            "Training for time step 6/100, t = 0.0600\n",
            "  Time step 6, Batch 500/2000, Loss: 0.000850\n",
            "  Time step 6, Batch 1000/2000, Loss: 0.000890\n",
            "  Time step 6, Batch 1500/2000, Loss: 0.000850\n",
            "  Time step 6, Batch 2000/2000, Loss: 0.000866\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.06.png\n",
            "\n",
            "Training for time step 7/100, t = 0.0700\n",
            "  Time step 7, Batch 500/2000, Loss: 0.000884\n",
            "  Time step 7, Batch 1000/2000, Loss: 0.000835\n",
            "  Time step 7, Batch 1500/2000, Loss: 0.000890\n",
            "  Time step 7, Batch 2000/2000, Loss: 0.000894\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.07.png\n",
            "\n",
            "Training for time step 8/100, t = 0.0800\n",
            "  Time step 8, Batch 500/2000, Loss: 0.000876\n",
            "  Time step 8, Batch 1000/2000, Loss: 0.000857\n",
            "  Time step 8, Batch 1500/2000, Loss: 0.000898\n",
            "  Time step 8, Batch 2000/2000, Loss: 0.000834\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.08.png\n",
            "\n",
            "Training for time step 9/100, t = 0.0900\n",
            "  Time step 9, Batch 500/2000, Loss: 0.000847\n",
            "  Time step 9, Batch 1000/2000, Loss: 0.000828\n",
            "  Time step 9, Batch 1500/2000, Loss: 0.000921\n",
            "  Time step 9, Batch 2000/2000, Loss: 0.000872\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.09.png\n",
            "\n",
            "Training for time step 10/100, t = 0.1000\n",
            "  Time step 10, Batch 500/2000, Loss: 0.000849\n",
            "  Time step 10, Batch 1000/2000, Loss: 0.000838\n",
            "  Time step 10, Batch 1500/2000, Loss: 0.000856\n",
            "  Time step 10, Batch 2000/2000, Loss: 0.000870\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.1.png\n",
            "\n",
            "Training for time step 11/100, t = 0.1100\n",
            "  Time step 11, Batch 500/2000, Loss: 0.000858\n",
            "  Time step 11, Batch 1000/2000, Loss: 0.000922\n",
            "  Time step 11, Batch 1500/2000, Loss: 0.000813\n",
            "  Time step 11, Batch 2000/2000, Loss: 0.000896\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.11.png\n",
            "\n",
            "Training for time step 12/100, t = 0.1200\n",
            "  Time step 12, Batch 500/2000, Loss: 0.000966\n",
            "  Time step 12, Batch 1000/2000, Loss: 0.000862\n",
            "  Time step 12, Batch 1500/2000, Loss: 0.000902\n",
            "  Time step 12, Batch 2000/2000, Loss: 0.000972\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.12.png\n",
            "\n",
            "Training for time step 13/100, t = 0.1300\n",
            "  Time step 13, Batch 500/2000, Loss: 0.000842\n",
            "  Time step 13, Batch 1000/2000, Loss: 0.000883\n",
            "  Time step 13, Batch 1500/2000, Loss: 0.000876\n",
            "  Time step 13, Batch 2000/2000, Loss: 0.000966\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.13.png\n",
            "\n",
            "Training for time step 14/100, t = 0.1400\n",
            "  Time step 14, Batch 500/2000, Loss: 0.000860\n",
            "  Time step 14, Batch 1000/2000, Loss: 0.000861\n",
            "  Time step 14, Batch 1500/2000, Loss: 0.000879\n",
            "  Time step 14, Batch 2000/2000, Loss: 0.000838\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.14.png\n",
            "\n",
            "Training for time step 15/100, t = 0.1500\n",
            "  Time step 15, Batch 500/2000, Loss: 0.000849\n",
            "  Time step 15, Batch 1000/2000, Loss: 0.000764\n",
            "  Time step 15, Batch 1500/2000, Loss: 0.000854\n",
            "  Time step 15, Batch 2000/2000, Loss: 0.000848\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.15.png\n",
            "\n",
            "Training for time step 16/100, t = 0.1600\n",
            "  Time step 16, Batch 500/2000, Loss: 0.000823\n",
            "  Time step 16, Batch 1000/2000, Loss: 0.000919\n",
            "  Time step 16, Batch 1500/2000, Loss: 0.000869\n",
            "  Time step 16, Batch 2000/2000, Loss: 0.000918\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.16.png\n",
            "\n",
            "Training for time step 17/100, t = 0.1700\n",
            "  Time step 17, Batch 500/2000, Loss: 0.000853\n",
            "  Time step 17, Batch 1000/2000, Loss: 0.000846\n",
            "  Time step 17, Batch 1500/2000, Loss: 0.000827\n",
            "  Time step 17, Batch 2000/2000, Loss: 0.000863\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.17.png\n",
            "\n",
            "Training for time step 18/100, t = 0.1800\n",
            "  Time step 18, Batch 500/2000, Loss: 0.000865\n",
            "  Time step 18, Batch 1000/2000, Loss: 0.000881\n",
            "  Time step 18, Batch 1500/2000, Loss: 0.000923\n",
            "  Time step 18, Batch 2000/2000, Loss: 0.000866\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.18.png\n",
            "\n",
            "Training for time step 19/100, t = 0.1900\n",
            "  Time step 19, Batch 500/2000, Loss: 0.000864\n",
            "  Time step 19, Batch 1000/2000, Loss: 0.000900\n",
            "  Time step 19, Batch 1500/2000, Loss: 0.000883\n",
            "  Time step 19, Batch 2000/2000, Loss: 0.000903\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.19.png\n",
            "\n",
            "Training for time step 20/100, t = 0.2000\n",
            "  Time step 20, Batch 500/2000, Loss: 0.000816\n",
            "  Time step 20, Batch 1000/2000, Loss: 0.000918\n",
            "  Time step 20, Batch 1500/2000, Loss: 0.000847\n",
            "  Time step 20, Batch 2000/2000, Loss: 0.000883\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.2.png\n",
            "\n",
            "Training for time step 21/100, t = 0.2100\n",
            "  Time step 21, Batch 500/2000, Loss: 0.000843\n",
            "  Time step 21, Batch 1000/2000, Loss: 0.000914\n",
            "  Time step 21, Batch 1500/2000, Loss: 0.000869\n",
            "  Time step 21, Batch 2000/2000, Loss: 0.000813\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.21.png\n",
            "\n",
            "Training for time step 22/100, t = 0.2200\n",
            "  Time step 22, Batch 500/2000, Loss: 0.000904\n",
            "  Time step 22, Batch 1000/2000, Loss: 0.000990\n",
            "  Time step 22, Batch 1500/2000, Loss: 0.000890\n",
            "  Time step 22, Batch 2000/2000, Loss: 0.000929\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.22.png\n",
            "\n",
            "Training for time step 23/100, t = 0.2300\n",
            "  Time step 23, Batch 500/2000, Loss: 0.000873\n",
            "  Time step 23, Batch 1000/2000, Loss: 0.000830\n",
            "  Time step 23, Batch 1500/2000, Loss: 0.000943\n",
            "  Time step 23, Batch 2000/2000, Loss: 0.000851\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.23.png\n",
            "\n",
            "Training for time step 24/100, t = 0.2400\n",
            "  Time step 24, Batch 500/2000, Loss: 0.000872\n",
            "  Time step 24, Batch 1000/2000, Loss: 0.000887\n",
            "  Time step 24, Batch 1500/2000, Loss: 0.000886\n",
            "  Time step 24, Batch 2000/2000, Loss: 0.000833\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.24.png\n",
            "\n",
            "Training for time step 25/100, t = 0.2500\n",
            "  Time step 25, Batch 500/2000, Loss: 0.000894\n",
            "  Time step 25, Batch 1000/2000, Loss: 0.000903\n",
            "  Time step 25, Batch 1500/2000, Loss: 0.000918\n",
            "  Time step 25, Batch 2000/2000, Loss: 0.000838\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.25.png\n",
            "\n",
            "Training for time step 26/100, t = 0.2600\n",
            "  Time step 26, Batch 500/2000, Loss: 0.000855\n",
            "  Time step 26, Batch 1000/2000, Loss: 0.000855\n",
            "  Time step 26, Batch 1500/2000, Loss: 0.000895\n",
            "  Time step 26, Batch 2000/2000, Loss: 0.000937\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.26.png\n",
            "\n",
            "Training for time step 27/100, t = 0.2700\n",
            "  Time step 27, Batch 500/2000, Loss: 0.000849\n",
            "  Time step 27, Batch 1000/2000, Loss: 0.000883\n",
            "  Time step 27, Batch 1500/2000, Loss: 0.000926\n",
            "  Time step 27, Batch 2000/2000, Loss: 0.000882\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.27.png\n",
            "\n",
            "Training for time step 28/100, t = 0.2800\n",
            "  Time step 28, Batch 500/2000, Loss: 0.000809\n",
            "  Time step 28, Batch 1000/2000, Loss: 0.000831\n",
            "  Time step 28, Batch 1500/2000, Loss: 0.000875\n",
            "  Time step 28, Batch 2000/2000, Loss: 0.000849\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.28.png\n",
            "\n",
            "Training for time step 29/100, t = 0.2900\n",
            "  Time step 29, Batch 500/2000, Loss: 0.000849\n",
            "  Time step 29, Batch 1000/2000, Loss: 0.000958\n",
            "  Time step 29, Batch 1500/2000, Loss: 0.000919\n",
            "  Time step 29, Batch 2000/2000, Loss: 0.000935\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.29.png\n",
            "\n",
            "Training for time step 30/100, t = 0.3000\n",
            "  Time step 30, Batch 500/2000, Loss: 0.000902\n",
            "  Time step 30, Batch 1000/2000, Loss: 0.000860\n",
            "  Time step 30, Batch 1500/2000, Loss: 0.000945\n",
            "  Time step 30, Batch 2000/2000, Loss: 0.000937\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.3.png\n",
            "\n",
            "Training for time step 31/100, t = 0.3100\n",
            "  Time step 31, Batch 500/2000, Loss: 0.000816\n",
            "  Time step 31, Batch 1000/2000, Loss: 0.000890\n",
            "  Time step 31, Batch 1500/2000, Loss: 0.000853\n",
            "  Time step 31, Batch 2000/2000, Loss: 0.000951\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.31.png\n",
            "\n",
            "Training for time step 32/100, t = 0.3200\n",
            "  Time step 32, Batch 500/2000, Loss: 0.000913\n",
            "  Time step 32, Batch 1000/2000, Loss: 0.000824\n",
            "  Time step 32, Batch 1500/2000, Loss: 0.000939\n",
            "  Time step 32, Batch 2000/2000, Loss: 0.000920\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.32.png\n",
            "\n",
            "Training for time step 33/100, t = 0.3300\n",
            "  Time step 33, Batch 500/2000, Loss: 0.000916\n",
            "  Time step 33, Batch 1000/2000, Loss: 0.000845\n",
            "  Time step 33, Batch 1500/2000, Loss: 0.000872\n",
            "  Time step 33, Batch 2000/2000, Loss: 0.000915\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.33.png\n",
            "\n",
            "Training for time step 34/100, t = 0.3400\n",
            "  Time step 34, Batch 500/2000, Loss: 0.000938\n",
            "  Time step 34, Batch 1000/2000, Loss: 0.000871\n",
            "  Time step 34, Batch 1500/2000, Loss: 0.000858\n",
            "  Time step 34, Batch 2000/2000, Loss: 0.000824\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.34.png\n",
            "\n",
            "Training for time step 35/100, t = 0.3500\n",
            "  Time step 35, Batch 500/2000, Loss: 0.000953\n",
            "  Time step 35, Batch 1000/2000, Loss: 0.000872\n",
            "  Time step 35, Batch 1500/2000, Loss: 0.000821\n",
            "  Time step 35, Batch 2000/2000, Loss: 0.000839\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.35.png\n",
            "\n",
            "Training for time step 36/100, t = 0.3600\n",
            "  Time step 36, Batch 500/2000, Loss: 0.000863\n",
            "  Time step 36, Batch 1000/2000, Loss: 0.000864\n",
            "  Time step 36, Batch 1500/2000, Loss: 0.000875\n",
            "  Time step 36, Batch 2000/2000, Loss: 0.000915\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.36.png\n",
            "\n",
            "Training for time step 37/100, t = 0.3700\n",
            "  Time step 37, Batch 500/2000, Loss: 0.000946\n",
            "  Time step 37, Batch 1000/2000, Loss: 0.000892\n",
            "  Time step 37, Batch 1500/2000, Loss: 0.000875\n",
            "  Time step 37, Batch 2000/2000, Loss: 0.000861\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.37.png\n",
            "\n",
            "Training for time step 38/100, t = 0.3800\n",
            "  Time step 38, Batch 500/2000, Loss: 0.000874\n",
            "  Time step 38, Batch 1000/2000, Loss: 0.000888\n",
            "  Time step 38, Batch 1500/2000, Loss: 0.000854\n",
            "  Time step 38, Batch 2000/2000, Loss: 0.000899\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.38.png\n",
            "\n",
            "Training for time step 39/100, t = 0.3900\n",
            "  Time step 39, Batch 500/2000, Loss: 0.000891\n",
            "  Time step 39, Batch 1000/2000, Loss: 0.000842\n",
            "  Time step 39, Batch 1500/2000, Loss: 0.000904\n",
            "  Time step 39, Batch 2000/2000, Loss: 0.000894\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.39.png\n",
            "\n",
            "Training for time step 40/100, t = 0.4000\n",
            "  Time step 40, Batch 500/2000, Loss: 0.000792\n",
            "  Time step 40, Batch 1000/2000, Loss: 0.000910\n",
            "  Time step 40, Batch 1500/2000, Loss: 0.000841\n",
            "  Time step 40, Batch 2000/2000, Loss: 0.000891\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.4.png\n",
            "\n",
            "Training for time step 41/100, t = 0.4100\n",
            "  Time step 41, Batch 500/2000, Loss: 0.000858\n",
            "  Time step 41, Batch 1000/2000, Loss: 0.000918\n",
            "  Time step 41, Batch 1500/2000, Loss: 0.000912\n",
            "  Time step 41, Batch 2000/2000, Loss: 0.000905\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.41.png\n",
            "\n",
            "Training for time step 42/100, t = 0.4200\n",
            "  Time step 42, Batch 500/2000, Loss: 0.000857\n",
            "  Time step 42, Batch 1000/2000, Loss: 0.000851\n",
            "  Time step 42, Batch 1500/2000, Loss: 0.000850\n",
            "  Time step 42, Batch 2000/2000, Loss: 0.000863\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.42.png\n",
            "\n",
            "Training for time step 43/100, t = 0.4300\n",
            "  Time step 43, Batch 500/2000, Loss: 0.000906\n",
            "  Time step 43, Batch 1000/2000, Loss: 0.000885\n",
            "  Time step 43, Batch 1500/2000, Loss: 0.000862\n",
            "  Time step 43, Batch 2000/2000, Loss: 0.000820\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.43.png\n",
            "\n",
            "Training for time step 44/100, t = 0.4400\n",
            "  Time step 44, Batch 500/2000, Loss: 0.000837\n",
            "  Time step 44, Batch 1000/2000, Loss: 0.000916\n",
            "  Time step 44, Batch 1500/2000, Loss: 0.000891\n",
            "  Time step 44, Batch 2000/2000, Loss: 0.000951\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.44.png\n",
            "\n",
            "Training for time step 45/100, t = 0.4500\n",
            "  Time step 45, Batch 500/2000, Loss: 0.000931\n",
            "  Time step 45, Batch 1000/2000, Loss: 0.000905\n",
            "  Time step 45, Batch 1500/2000, Loss: 0.000956\n",
            "  Time step 45, Batch 2000/2000, Loss: 0.000857\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.45.png\n",
            "\n",
            "Training for time step 46/100, t = 0.4600\n",
            "  Time step 46, Batch 500/2000, Loss: 0.000887\n",
            "  Time step 46, Batch 1000/2000, Loss: 0.000901\n",
            "  Time step 46, Batch 1500/2000, Loss: 0.000950\n",
            "  Time step 46, Batch 2000/2000, Loss: 0.000928\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.46.png\n",
            "\n",
            "Training for time step 47/100, t = 0.4700\n",
            "  Time step 47, Batch 500/2000, Loss: 0.000943\n",
            "  Time step 47, Batch 1000/2000, Loss: 0.000910\n",
            "  Time step 47, Batch 1500/2000, Loss: 0.000871\n",
            "  Time step 47, Batch 2000/2000, Loss: 0.000931\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.47.png\n",
            "\n",
            "Training for time step 48/100, t = 0.4800\n",
            "  Time step 48, Batch 500/2000, Loss: 0.000908\n",
            "  Time step 48, Batch 1000/2000, Loss: 0.000924\n",
            "  Time step 48, Batch 1500/2000, Loss: 0.000873\n",
            "  Time step 48, Batch 2000/2000, Loss: 0.000868\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.48.png\n",
            "\n",
            "Training for time step 49/100, t = 0.4900\n",
            "  Time step 49, Batch 500/2000, Loss: 0.000872\n",
            "  Time step 49, Batch 1000/2000, Loss: 0.000883\n",
            "  Time step 49, Batch 1500/2000, Loss: 0.000896\n",
            "  Time step 49, Batch 2000/2000, Loss: 0.000840\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.49.png\n",
            "\n",
            "Training for time step 50/100, t = 0.5000\n",
            "  Time step 50, Batch 500/2000, Loss: 0.000914\n",
            "  Time step 50, Batch 1000/2000, Loss: 0.000885\n",
            "  Time step 50, Batch 1500/2000, Loss: 0.000928\n",
            "  Time step 50, Batch 2000/2000, Loss: 0.000896\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.5.png\n",
            "\n",
            "Training for time step 51/100, t = 0.5100\n",
            "  Time step 51, Batch 500/2000, Loss: 0.000855\n",
            "  Time step 51, Batch 1000/2000, Loss: 0.000871\n",
            "  Time step 51, Batch 1500/2000, Loss: 0.000864\n",
            "  Time step 51, Batch 2000/2000, Loss: 0.000851\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.51.png\n",
            "\n",
            "Training for time step 52/100, t = 0.5200\n",
            "  Time step 52, Batch 500/2000, Loss: 0.000866\n",
            "  Time step 52, Batch 1000/2000, Loss: 0.000840\n",
            "  Time step 52, Batch 1500/2000, Loss: 0.000875\n",
            "  Time step 52, Batch 2000/2000, Loss: 0.000887\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.52.png\n",
            "\n",
            "Training for time step 53/100, t = 0.5300\n",
            "  Time step 53, Batch 500/2000, Loss: 0.000859\n",
            "  Time step 53, Batch 1000/2000, Loss: 0.000869\n",
            "  Time step 53, Batch 1500/2000, Loss: 0.000853\n",
            "  Time step 53, Batch 2000/2000, Loss: 0.000933\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.53.png\n",
            "\n",
            "Training for time step 54/100, t = 0.5400\n",
            "  Time step 54, Batch 500/2000, Loss: 0.000851\n",
            "  Time step 54, Batch 1000/2000, Loss: 0.000975\n",
            "  Time step 54, Batch 1500/2000, Loss: 0.000996\n",
            "  Time step 54, Batch 2000/2000, Loss: 0.000904\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.54.png\n",
            "\n",
            "Training for time step 55/100, t = 0.5500\n",
            "  Time step 55, Batch 500/2000, Loss: 0.000922\n",
            "  Time step 55, Batch 1000/2000, Loss: 0.000851\n",
            "  Time step 55, Batch 1500/2000, Loss: 0.000874\n",
            "  Time step 55, Batch 2000/2000, Loss: 0.000916\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.55.png\n",
            "\n",
            "Training for time step 56/100, t = 0.5600\n",
            "  Time step 56, Batch 500/2000, Loss: 0.000847\n",
            "  Time step 56, Batch 1000/2000, Loss: 0.000933\n",
            "  Time step 56, Batch 1500/2000, Loss: 0.000873\n",
            "  Time step 56, Batch 2000/2000, Loss: 0.000905\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.56.png\n",
            "\n",
            "Training for time step 57/100, t = 0.5700\n",
            "  Time step 57, Batch 500/2000, Loss: 0.000820\n",
            "  Time step 57, Batch 1000/2000, Loss: 0.000883\n",
            "  Time step 57, Batch 1500/2000, Loss: 0.000892\n",
            "  Time step 57, Batch 2000/2000, Loss: 0.000916\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.57.png\n",
            "\n",
            "Training for time step 58/100, t = 0.5800\n",
            "  Time step 58, Batch 500/2000, Loss: 0.000925\n",
            "  Time step 58, Batch 1000/2000, Loss: 0.000839\n",
            "  Time step 58, Batch 1500/2000, Loss: 0.000904\n",
            "  Time step 58, Batch 2000/2000, Loss: 0.000941\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.58.png\n",
            "\n",
            "Training for time step 59/100, t = 0.5900\n",
            "  Time step 59, Batch 500/2000, Loss: 0.000870\n",
            "  Time step 59, Batch 1000/2000, Loss: 0.000842\n",
            "  Time step 59, Batch 1500/2000, Loss: 0.000870\n",
            "  Time step 59, Batch 2000/2000, Loss: 0.000922\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.59.png\n",
            "\n",
            "Training for time step 60/100, t = 0.6000\n",
            "  Time step 60, Batch 500/2000, Loss: 0.000860\n",
            "  Time step 60, Batch 1000/2000, Loss: 0.000894\n",
            "  Time step 60, Batch 1500/2000, Loss: 0.000898\n",
            "  Time step 60, Batch 2000/2000, Loss: 0.000862\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.6.png\n",
            "\n",
            "Training for time step 61/100, t = 0.6100\n",
            "  Time step 61, Batch 500/2000, Loss: 0.000902\n",
            "  Time step 61, Batch 1000/2000, Loss: 0.000886\n",
            "  Time step 61, Batch 1500/2000, Loss: 0.000908\n",
            "  Time step 61, Batch 2000/2000, Loss: 0.000942\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.61.png\n",
            "\n",
            "Training for time step 62/100, t = 0.6200\n",
            "  Time step 62, Batch 500/2000, Loss: 0.000863\n",
            "  Time step 62, Batch 1000/2000, Loss: 0.000901\n",
            "  Time step 62, Batch 1500/2000, Loss: 0.000897\n",
            "  Time step 62, Batch 2000/2000, Loss: 0.000924\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.62.png\n",
            "\n",
            "Training for time step 63/100, t = 0.6300\n",
            "  Time step 63, Batch 500/2000, Loss: 0.000939\n",
            "  Time step 63, Batch 1000/2000, Loss: 0.000942\n",
            "  Time step 63, Batch 1500/2000, Loss: 0.000832\n",
            "  Time step 63, Batch 2000/2000, Loss: 0.000842\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.63.png\n",
            "\n",
            "Training for time step 64/100, t = 0.6400\n",
            "  Time step 64, Batch 500/2000, Loss: 0.000866\n",
            "  Time step 64, Batch 1000/2000, Loss: 0.000841\n",
            "  Time step 64, Batch 1500/2000, Loss: 0.000933\n",
            "  Time step 64, Batch 2000/2000, Loss: 0.000898\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.64.png\n",
            "\n",
            "Training for time step 65/100, t = 0.6500\n",
            "  Time step 65, Batch 500/2000, Loss: 0.000842\n",
            "  Time step 65, Batch 1000/2000, Loss: 0.000932\n",
            "  Time step 65, Batch 1500/2000, Loss: 0.000896\n",
            "  Time step 65, Batch 2000/2000, Loss: 0.000844\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.65.png\n",
            "\n",
            "Training for time step 66/100, t = 0.6600\n",
            "  Time step 66, Batch 500/2000, Loss: 0.000920\n",
            "  Time step 66, Batch 1000/2000, Loss: 0.000846\n",
            "  Time step 66, Batch 1500/2000, Loss: 0.000860\n",
            "  Time step 66, Batch 2000/2000, Loss: 0.000897\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.66.png\n",
            "\n",
            "Training for time step 67/100, t = 0.6700\n",
            "  Time step 67, Batch 500/2000, Loss: 0.000870\n",
            "  Time step 67, Batch 1000/2000, Loss: 0.000914\n",
            "  Time step 67, Batch 1500/2000, Loss: 0.000868\n",
            "  Time step 67, Batch 2000/2000, Loss: 0.000830\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.67.png\n",
            "\n",
            "Training for time step 68/100, t = 0.6800\n",
            "  Time step 68, Batch 500/2000, Loss: 0.000871\n",
            "  Time step 68, Batch 1000/2000, Loss: 0.000912\n",
            "  Time step 68, Batch 1500/2000, Loss: 0.000893\n",
            "  Time step 68, Batch 2000/2000, Loss: 0.000876\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.68.png\n",
            "\n",
            "Training for time step 69/100, t = 0.6900\n",
            "  Time step 69, Batch 500/2000, Loss: 0.000880\n",
            "  Time step 69, Batch 1000/2000, Loss: 0.000866\n",
            "  Time step 69, Batch 1500/2000, Loss: 0.000973\n",
            "  Time step 69, Batch 2000/2000, Loss: 0.000851\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.69.png\n",
            "\n",
            "Training for time step 70/100, t = 0.7000\n",
            "  Time step 70, Batch 500/2000, Loss: 0.000845\n",
            "  Time step 70, Batch 1000/2000, Loss: 0.000844\n",
            "  Time step 70, Batch 1500/2000, Loss: 0.000907\n",
            "  Time step 70, Batch 2000/2000, Loss: 0.000882\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.7.png\n",
            "\n",
            "Training for time step 71/100, t = 0.7100\n",
            "  Time step 71, Batch 500/2000, Loss: 0.000833\n",
            "  Time step 71, Batch 1000/2000, Loss: 0.000862\n",
            "  Time step 71, Batch 1500/2000, Loss: 0.000864\n",
            "  Time step 71, Batch 2000/2000, Loss: 0.000926\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.71.png\n",
            "\n",
            "Training for time step 72/100, t = 0.7200\n",
            "  Time step 72, Batch 500/2000, Loss: 0.000908\n",
            "  Time step 72, Batch 1000/2000, Loss: 0.000898\n",
            "  Time step 72, Batch 1500/2000, Loss: 0.000969\n",
            "  Time step 72, Batch 2000/2000, Loss: 0.000941\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.72.png\n",
            "\n",
            "Training for time step 73/100, t = 0.7300\n",
            "  Time step 73, Batch 500/2000, Loss: 0.000928\n",
            "  Time step 73, Batch 1000/2000, Loss: 0.000905\n",
            "  Time step 73, Batch 1500/2000, Loss: 0.000831\n",
            "  Time step 73, Batch 2000/2000, Loss: 0.000918\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.73.png\n",
            "\n",
            "Training for time step 74/100, t = 0.7400\n",
            "  Time step 74, Batch 500/2000, Loss: 0.000845\n",
            "  Time step 74, Batch 1000/2000, Loss: 0.000924\n",
            "  Time step 74, Batch 1500/2000, Loss: 0.000909\n",
            "  Time step 74, Batch 2000/2000, Loss: 0.000986\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.74.png\n",
            "\n",
            "Training for time step 75/100, t = 0.7500\n",
            "  Time step 75, Batch 500/2000, Loss: 0.000883\n",
            "  Time step 75, Batch 1000/2000, Loss: 0.000869\n",
            "  Time step 75, Batch 1500/2000, Loss: 0.000901\n",
            "  Time step 75, Batch 2000/2000, Loss: 0.000923\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.75.png\n",
            "\n",
            "Training for time step 76/100, t = 0.7600\n",
            "  Time step 76, Batch 500/2000, Loss: 0.000888\n",
            "  Time step 76, Batch 1000/2000, Loss: 0.001013\n",
            "  Time step 76, Batch 1500/2000, Loss: 0.000879\n",
            "  Time step 76, Batch 2000/2000, Loss: 0.000913\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.76.png\n",
            "\n",
            "Training for time step 77/100, t = 0.7700\n",
            "  Time step 77, Batch 500/2000, Loss: 0.000892\n",
            "  Time step 77, Batch 1000/2000, Loss: 0.000985\n",
            "  Time step 77, Batch 1500/2000, Loss: 0.000839\n",
            "  Time step 77, Batch 2000/2000, Loss: 0.000840\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.77.png\n",
            "\n",
            "Training for time step 78/100, t = 0.7800\n",
            "  Time step 78, Batch 500/2000, Loss: 0.000882\n",
            "  Time step 78, Batch 1000/2000, Loss: 0.000890\n",
            "  Time step 78, Batch 1500/2000, Loss: 0.000863\n",
            "  Time step 78, Batch 2000/2000, Loss: 0.000942\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.78.png\n",
            "\n",
            "Training for time step 79/100, t = 0.7900\n",
            "  Time step 79, Batch 500/2000, Loss: 0.000874\n",
            "  Time step 79, Batch 1000/2000, Loss: 0.000972\n",
            "  Time step 79, Batch 1500/2000, Loss: 0.000820\n",
            "  Time step 79, Batch 2000/2000, Loss: 0.000834\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.79.png\n",
            "\n",
            "Training for time step 80/100, t = 0.8000\n",
            "  Time step 80, Batch 500/2000, Loss: 0.000826\n",
            "  Time step 80, Batch 1000/2000, Loss: 0.000919\n",
            "  Time step 80, Batch 1500/2000, Loss: 0.000856\n",
            "  Time step 80, Batch 2000/2000, Loss: 0.000824\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.8.png\n",
            "\n",
            "Training for time step 81/100, t = 0.8100\n",
            "  Time step 81, Batch 500/2000, Loss: 0.000864\n",
            "  Time step 81, Batch 1000/2000, Loss: 0.000870\n",
            "  Time step 81, Batch 1500/2000, Loss: 0.000903\n",
            "  Time step 81, Batch 2000/2000, Loss: 0.000922\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.81.png\n",
            "\n",
            "Training for time step 82/100, t = 0.8200\n",
            "  Time step 82, Batch 500/2000, Loss: 0.000925\n",
            "  Time step 82, Batch 1000/2000, Loss: 0.000986\n",
            "  Time step 82, Batch 1500/2000, Loss: 0.000913\n",
            "  Time step 82, Batch 2000/2000, Loss: 0.000901\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.82.png\n",
            "\n",
            "Training for time step 83/100, t = 0.8300\n",
            "  Time step 83, Batch 500/2000, Loss: 0.000875\n",
            "  Time step 83, Batch 1000/2000, Loss: 0.000902\n",
            "  Time step 83, Batch 1500/2000, Loss: 0.000820\n",
            "  Time step 83, Batch 2000/2000, Loss: 0.000884\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.83.png\n",
            "\n",
            "Training for time step 84/100, t = 0.8400\n",
            "  Time step 84, Batch 500/2000, Loss: 0.000917\n",
            "  Time step 84, Batch 1000/2000, Loss: 0.000903\n",
            "  Time step 84, Batch 1500/2000, Loss: 0.000926\n",
            "  Time step 84, Batch 2000/2000, Loss: 0.000891\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.84.png\n",
            "\n",
            "Training for time step 85/100, t = 0.8500\n",
            "  Time step 85, Batch 500/2000, Loss: 0.000815\n",
            "  Time step 85, Batch 1000/2000, Loss: 0.000907\n",
            "  Time step 85, Batch 1500/2000, Loss: 0.000846\n",
            "  Time step 85, Batch 2000/2000, Loss: 0.000884\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.85.png\n",
            "\n",
            "Training for time step 86/100, t = 0.8600\n",
            "  Time step 86, Batch 500/2000, Loss: 0.000952\n",
            "  Time step 86, Batch 1000/2000, Loss: 0.000882\n",
            "  Time step 86, Batch 1500/2000, Loss: 0.000892\n",
            "  Time step 86, Batch 2000/2000, Loss: 0.000895\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.86.png\n",
            "\n",
            "Training for time step 87/100, t = 0.8700\n",
            "  Time step 87, Batch 500/2000, Loss: 0.000838\n",
            "  Time step 87, Batch 1000/2000, Loss: 0.000945\n",
            "  Time step 87, Batch 1500/2000, Loss: 0.000882\n",
            "  Time step 87, Batch 2000/2000, Loss: 0.000884\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.87.png\n",
            "\n",
            "Training for time step 88/100, t = 0.8800\n",
            "  Time step 88, Batch 500/2000, Loss: 0.000909\n",
            "  Time step 88, Batch 1000/2000, Loss: 0.000926\n",
            "  Time step 88, Batch 1500/2000, Loss: 0.000851\n",
            "  Time step 88, Batch 2000/2000, Loss: 0.000931\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.88.png\n",
            "\n",
            "Training for time step 89/100, t = 0.8900\n",
            "  Time step 89, Batch 500/2000, Loss: 0.000868\n",
            "  Time step 89, Batch 1000/2000, Loss: 0.000917\n",
            "  Time step 89, Batch 1500/2000, Loss: 0.000893\n",
            "  Time step 89, Batch 2000/2000, Loss: 0.000878\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.89.png\n",
            "\n",
            "Training for time step 90/100, t = 0.9000\n",
            "  Time step 90, Batch 500/2000, Loss: 0.000875\n",
            "  Time step 90, Batch 1000/2000, Loss: 0.000901\n",
            "  Time step 90, Batch 1500/2000, Loss: 0.000876\n",
            "  Time step 90, Batch 2000/2000, Loss: 0.000944\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.9.png\n",
            "\n",
            "Training for time step 91/100, t = 0.9100\n",
            "  Time step 91, Batch 500/2000, Loss: 0.000873\n",
            "  Time step 91, Batch 1000/2000, Loss: 0.000925\n",
            "  Time step 91, Batch 1500/2000, Loss: 0.000899\n",
            "  Time step 91, Batch 2000/2000, Loss: 0.000901\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.91.png\n",
            "\n",
            "Training for time step 92/100, t = 0.9200\n",
            "  Time step 92, Batch 500/2000, Loss: 0.000833\n",
            "  Time step 92, Batch 1000/2000, Loss: 0.000854\n",
            "  Time step 92, Batch 1500/2000, Loss: 0.000861\n",
            "  Time step 92, Batch 2000/2000, Loss: 0.000861\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.92.png\n",
            "\n",
            "Training for time step 93/100, t = 0.9300\n",
            "  Time step 93, Batch 500/2000, Loss: 0.000874\n",
            "  Time step 93, Batch 1000/2000, Loss: 0.000865\n",
            "  Time step 93, Batch 1500/2000, Loss: 0.000926\n",
            "  Time step 93, Batch 2000/2000, Loss: 0.000938\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.93.png\n",
            "\n",
            "Training for time step 94/100, t = 0.9400\n",
            "  Time step 94, Batch 500/2000, Loss: 0.000930\n",
            "  Time step 94, Batch 1000/2000, Loss: 0.000862\n",
            "  Time step 94, Batch 1500/2000, Loss: 0.000896\n",
            "  Time step 94, Batch 2000/2000, Loss: 0.000838\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.94.png\n",
            "\n",
            "Training for time step 95/100, t = 0.9500\n",
            "  Time step 95, Batch 500/2000, Loss: 0.000863\n",
            "  Time step 95, Batch 1000/2000, Loss: 0.000938\n",
            "  Time step 95, Batch 1500/2000, Loss: 0.000931\n",
            "  Time step 95, Batch 2000/2000, Loss: 0.000908\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.95.png\n",
            "\n",
            "Training for time step 96/100, t = 0.9600\n",
            "  Time step 96, Batch 500/2000, Loss: 0.000909\n",
            "  Time step 96, Batch 1000/2000, Loss: 0.000912\n",
            "  Time step 96, Batch 1500/2000, Loss: 0.000852\n",
            "  Time step 96, Batch 2000/2000, Loss: 0.000947\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.96.png\n",
            "\n",
            "Training for time step 97/100, t = 0.9700\n",
            "  Time step 97, Batch 500/2000, Loss: 0.000855\n",
            "  Time step 97, Batch 1000/2000, Loss: 0.000895\n",
            "  Time step 97, Batch 1500/2000, Loss: 0.000910\n",
            "  Time step 97, Batch 2000/2000, Loss: 0.000940\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.97.png\n",
            "\n",
            "Training for time step 98/100, t = 0.9800\n",
            "  Time step 98, Batch 500/2000, Loss: 0.000924\n",
            "  Time step 98, Batch 1000/2000, Loss: 0.000885\n",
            "  Time step 98, Batch 1500/2000, Loss: 0.000894\n",
            "  Time step 98, Batch 2000/2000, Loss: 0.000810\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.98.png\n",
            "\n",
            "Training for time step 99/100, t = 0.9900\n",
            "  Time step 99, Batch 500/2000, Loss: 0.000876\n",
            "  Time step 99, Batch 1000/2000, Loss: 0.000924\n",
            "  Time step 99, Batch 1500/2000, Loss: 0.000909\n",
            "  Time step 99, Batch 2000/2000, Loss: 0.000895\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=0.99.png\n",
            "\n",
            "Training for time step 100/100, t = 1.0000\n",
            "  Time step 100, Batch 500/2000, Loss: 0.000877\n",
            "  Time step 100, Batch 1000/2000, Loss: 0.000916\n",
            "  Time step 100, Batch 1500/2000, Loss: 0.000849\n",
            "  Time step 100, Batch 2000/2000, Loss: 0.000880\n",
            "Generating plot...\n",
            "Plot saved to plots/BS_call_t=1.0.png\n",
            "\n",
            "Time step training finished.\n",
            "Total training time: 2364.24 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "39 minutes\n",
        "\n"
      ],
      "metadata": {
        "id": "2oZKAw1cxGxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the strike price K (assuming it's 1.0 from the Phi function)\n",
        "K = 1.0\n",
        "\n",
        "# Generate evaluation points for S\n",
        "S_eval = torch.linspace(S_low, S_high, 1000).unsqueeze(1).to(device)\n",
        "\n",
        "mse_values = []\n",
        "mae_values = []\n",
        "times_evaluated = []\n",
        "\n",
        "print(\"\\nEvaluating MSE and MAE at different time steps...\")\n",
        "\n",
        "# Select 10 time steps for evaluation\n",
        "eval_indices = np.linspace(0, N_t - 1, 10, dtype=int)\n",
        "# Ensure the last time step is included\n",
        "if N_t - 1 not in eval_indices:\n",
        "    eval_indices = np.append(eval_indices, N_t - 1)\n",
        "eval_indices = np.unique(eval_indices) # Remove duplicates\n",
        "\n",
        "all_errors = []\n",
        "\n",
        "for i in eval_indices:\n",
        "    curr_t = (i + 1) * h # Calculate the time for the current time step\n",
        "    times_evaluated.append(curr_t)\n",
        "\n",
        "    # Load the model saved at this time step\n",
        "    filename_curr = 'weights/' + name + '_t=' + str(round(curr_t, 4)) # Match the saving filename format\n",
        "    try:\n",
        "        # Instantiate the model first\n",
        "        # These parameters should match your model initialization\n",
        "        # Ensure nodes_per_layer, d, r, T, CP, xp are defined and consistent\n",
        "        nodes_per_layer = 50 # Example value, replace with actual value\n",
        "        d = 1 # Example value, replace with actual value\n",
        "        r = 0.05 # Example value, replace with actual value\n",
        "        # T is already defined\n",
        "        CP = 1 # Example value, replace with actual value\n",
        "        xp = 2.0 # Example value, replace with actual value\n",
        "\n",
        "        # Instantiate the model with the correct parameters and move it to the device\n",
        "        loaded_model = ResidualNet(nodes_per_layer, d, r=r, t=curr_t, CP=CP, xp=xp).to(device)\n",
        "\n",
        "        # Load the state dictionary into the instantiated model\n",
        "        # Use map_location=device to ensure tensors are loaded onto the correct device\n",
        "        loaded_model.load_state_dict(torch.load(filename_curr, map_location=device))\n",
        "\n",
        "        loaded_model.eval() # Set the model to evaluation mode\n",
        "        # The time parameter is already set during instantiation, but can be re-set for clarity\n",
        "        loaded_model.t = curr_t\n",
        "\n",
        "        # Get the DGMNet prediction\n",
        "        with torch.no_grad():\n",
        "            # The forward pass expects *args. S_eval is the first argument.\n",
        "            # If your model needs time as an input (d=2 for t, S), you would need to\n",
        "            # provide both t and S to the forward pass, e.g., loaded_model(t_eval, S_eval)\n",
        "            # where t_eval is a tensor of current time values.\n",
        "            # Given d=1, the model likely only takes S.\n",
        "            dgm_price = loaded_model(S_eval)\n",
        "\n",
        "\n",
        "        # Calculate the Black-Scholes price at this time step\n",
        "        tau = T - curr_t\n",
        "        bs_price = black_scholes_price(S_eval, K, tau, T, r, sigma, option)\n",
        "\n",
        "        # Calculate errors\n",
        "        errors = dgm_price - bs_price\n",
        "        mse = torch.mean(errors**2).item()\n",
        "        mae = torch.mean(torch.abs(errors)).item()\n",
        "\n",
        "        mse_values.append(mse)\n",
        "        mae_values.append(mae)\n",
        "        all_errors.append(errors.cpu().numpy()) # Store errors for final calculation\n",
        "\n",
        "        # Print with scientific notation\n",
        "        print(f\"  Time t = {curr_t:.4f}: MSE = {mse:.6e}, MAE = {mae:.6e}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  Model file not found for time t = {curr_t:.4f}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  An error occurred for time t = {curr_t:.4f}: {e}\")\n",
        "\n",
        "\n",
        "# Calculate overall MSE and MAE\n",
        "if all_errors:\n",
        "    all_errors_combined = np.concatenate(all_errors)\n",
        "    final_mse = np.mean(all_errors_combined**2)\n",
        "    final_mae = np.mean(np.abs(all_errors_combined))\n",
        "\n",
        "    # Print with scientific notation\n",
        "    print(f\"\\nFinal Overall MSE (averaged over evaluated time steps): {final_mse:.6e}\")\n",
        "    print(f\"Final Overall MAE (averaged over evaluated time steps): {final_mae:.6e}\")\n",
        "else:\n",
        "    print(\"\\nNo model files were loaded for evaluation.\")"
      ],
      "metadata": {
        "id": "BdedhlAb3Eim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b468831a-4719-472e-a3e0-f629361af499"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating MSE and MAE at different time steps...\n",
            "  Time t = 0.0100: MSE = 8.030138e-07, MAE = 4.935141e-04\n",
            "  Time t = 0.1200: MSE = 2.853910e-06, MAE = 6.380667e-04\n",
            "  Time t = 0.2300: MSE = 1.573004e-06, MAE = 5.873141e-04\n",
            "  Time t = 0.3400: MSE = 2.083292e-06, MAE = 8.124989e-04\n",
            "  Time t = 0.4500: MSE = 2.370743e-06, MAE = 9.849584e-04\n",
            "  Time t = 0.5600: MSE = 1.882911e-06, MAE = 8.385243e-04\n",
            "  Time t = 0.6700: MSE = 2.760801e-06, MAE = 9.920127e-04\n",
            "  Time t = 0.7800: MSE = 3.114200e-06, MAE = 1.150759e-03\n",
            "  Time t = 0.8900: MSE = 2.221741e-06, MAE = 1.032430e-03\n",
            "  Time t = 1.0000: MSE = 3.401763e-06, MAE = 1.275177e-03\n",
            "\n",
            "Final Overall MSE (averaged over evaluated time steps): 2.306538e-06\n",
            "Final Overall MAE (averaged over evaluated time steps): 8.805254e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Final Overall MSE (averaged over evaluated time steps): 2.306538e-06\n",
        "\n",
        "Final Overall MAE (averaged over evaluated time steps): 8.805254e-04\n"
      ],
      "metadata": {
        "id": "ueLCaBcOVTUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the folders you want to download\n",
        "folders_to_download = ['plots', 'sample_data', 'weights']\n",
        "\n",
        "for folder_name in folders_to_download:\n",
        "    # Check if the folder exists\n",
        "    if os.path.exists(folder_name):\n",
        "        # Create a zip file of the folder\n",
        "        zip_filename = f'{folder_name}.zip'\n",
        "        !zip -r \"{zip_filename}\" \"{folder_name}\"\n",
        "\n",
        "        # Download the zip file\n",
        "        try:\n",
        "            files.download(zip_filename)\n",
        "            print(f\"Downloaded {zip_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {zip_filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_name}' not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1WxgOlYQ2uXz",
        "outputId": "396ac5d9-8daa-44a9-d758-adea6155865c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: plots/ (stored 0%)\n",
            "  adding: plots/BS_call_t=0.1.png (deflated 12%)\n",
            "  adding: plots/BS_call_t=0.65.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.03.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.67.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.71.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.27.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.53.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.79.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.23.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.72.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.25.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.66.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.84.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.64.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.2.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.8.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.59.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.97.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.35.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.55.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.38.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.74.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.32.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.16.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.05.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.91.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.87.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.85.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.94.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.46.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.02.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.52.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.61.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.41.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.56.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.26.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.39.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.15.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.48.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.62.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.83.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.63.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.77.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.88.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.81.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.17.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.09.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.92.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.89.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.69.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.28.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.22.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.01.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.11.png (deflated 12%)\n",
            "  adding: plots/BS_call_t=0.76.png (deflated 12%)\n",
            "  adding: plots/BS_call_t=0.4.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.54.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.57.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.08.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.13.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.37.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.49.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.96.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.68.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.47.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.31.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.45.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.3.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.04.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.34.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.14.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.29.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.19.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.21.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.44.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.7.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.36.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.95.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.75.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.12.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.98.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.6.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.5.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.78.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.24.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.99.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.73.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.33.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.86.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.93.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.18.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.51.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.42.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.58.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.9.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.43.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.82.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=1.0.png (deflated 10%)\n",
            "  adding: plots/BS_call_t=0.06.png (deflated 11%)\n",
            "  adding: plots/BS_call_t=0.07.png (deflated 11%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ff9d695-91c6-4a71-8b83-4d5036612b60\", \"plots.zip\", 3758629)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded plots.zip\n",
            "  adding: sample_data/ (stored 0%)\n",
            "  adding: sample_data/README.md (deflated 39%)\n",
            "  adding: sample_data/anscombe.json (deflated 83%)\n",
            "  adding: sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: sample_data/mnist_test.csv (deflated 88%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_57901c05-4555-4f41-9f3d-31bf233eef1b\", \"sample_data.zip\", 7063971)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded sample_data.zip\n",
            "  adding: weights/ (stored 0%)\n",
            "  adding: weights/BS_call_t=0.87 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.05 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.44 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.17 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.09 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.81 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.51 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.86 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.89 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.9 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.64 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.63 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.01 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.88 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.16 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.85 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.07 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.66 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.95 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.92 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.37 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.91 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.59 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.04 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.57 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.65 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.55 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.72 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.49 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.82 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.74 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.19 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.99 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.52 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.3 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.24 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.46 (deflated 13%)\n",
            "  adding: weights/BS_call_t=1.0 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.47 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.35 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.62 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.38 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.03 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.39 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.31 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.29 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.26 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.43 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.21 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.41 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.33 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.13 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.96 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.5 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.54 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.8 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.36 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.23 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.84 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.67 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.58 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.97 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.53 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.2 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.14 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.93 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.22 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.06 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.75 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.11 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.0 (deflated 14%)\n",
            "  adding: weights/BS_call_t=0.48 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.77 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.83 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.1 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.34 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.61 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.94 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.68 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.28 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.18 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.42 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.45 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.73 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.69 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.6 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.02 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.25 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.32 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.79 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.56 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.78 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.15 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.27 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.08 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.71 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.98 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.76 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.7 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.12 (deflated 13%)\n",
            "  adding: weights/BS_call_t=0.4 (deflated 13%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b75605a3-260d-4975-bbba-c235918699eb\", \"weights.zip\", 3070824)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded weights.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49U8GejS3WH9"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}